{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1Lw+1OkKLV6sGdzeSLjjY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**# Conversational RAG**"
      ],
      "metadata": {
        "id": "j4s_TK9wCKe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\n"
      ],
      "metadata": {
        "id": "C0CeyLZ7G-iI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will cover two approaches:\n",
        "\n",
        "- **Chains,** in which we always execute a retrieval step;\n",
        "- **Agents,** in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).\n"
      ],
      "metadata": {
        "id": "KbhDUyceHDWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "5gTyDGwXHMZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-VG7dSwG9ME",
        "outputId": "6c4cbf2c-f0fd-4919-e34a-c07fe472a35b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.6/315.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/asgiref/\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "9Vq6RvxRHhNM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ[\"COHERE_API_KEY\"]:\n",
        "    os.environ[\"COHERE_API_KEY\"]=userdata.get('COHERE_API_KEY')\n",
        "if not os.environ[\"LANGCHAIN_API_KEY\"]:\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"]=userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=\"Conversational-RAG\""
      ],
      "metadata": {
        "id": "Y8jtNbhnJWCa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "3skyl4Z2KWg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip uninstall -y transformers\n",
        "! pip install -q langchain-cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXcCQEZHJsIq",
        "outputId": "739d8d8f-daa3-4d76-d217-0bd19f27b72a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.41.2\n",
            "Uninstalling transformers-4.41.2:\n",
            "  Successfully uninstalled transformers-4.41.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader # pdf loader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # splitter\n",
        "from langchain_chroma import Chroma # vectorstore\n",
        "from langchain_cohere import CohereEmbeddings # embeddings\n",
        "from langchain.chains import create_retrieval_chain # chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain # chain\n",
        "from langchain_core.prompts import ChatPromptTemplate # prompt\n"
      ],
      "metadata": {
        "id": "dIs7gZ3MM00I"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet bs4"
      ],
      "metadata": {
        "id": "aCbwUIZ2ciZs"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "# # 1. Load data\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "                       bs_kwargs=dict(\n",
        "                           parse_only=bs4.SoupStrainer(\n",
        "                               class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "                           )\n",
        "                       )\n",
        "                       )\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "J8gIepDmKSFv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. chunk documents\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "doc_splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "GNJP44e6Q9oF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc_splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2npRaiFd3pD",
        "outputId": "ace08cc4-4d5b-49b7-c511-450564afb7b5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_splits[13].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "JvEjvC6-d8UB",
        "outputId": "5b82490f-20c7-4f1a-cb67-05f3deecfd97"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. index the contents of the blog to create a retriever.\n",
        "vectorstore = Chroma.from_documents(documents=doc_splits, embedding=CohereEmbeddings())\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "CImItyuAMnrr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "htZcOGQQYZWm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. LLM model\n",
        "from langchain_cohere import ChatCohere\n",
        "model = ChatCohere(model=\"command-r\")"
      ],
      "metadata": {
        "id": "RzWiuIHpg0tg"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Prompt template\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answerthe question.\n",
        "If you don't know the answer, say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\\n\\n\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', system_prompt),\n",
        "        ('human', '{input}')\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm=model, prompt=prompt)\n",
        "rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=question_answer_chain)"
      ],
      "metadata": {
        "id": "WSxVzsi5eiAH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "respone = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
        "respone['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "rwBZ_xh5iQxv",
        "outputId": "3be2027b-3a28-4380-fa90-dc23a3fb53cb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is a process that breaks down complex tasks into simpler, more manageable steps. It's a crucial component of planning in autonomous agent systems, enabling them to tackle complicated assignments by first understanding and decomposing them into clear, executable actions. This technique enhances the agent's ability to tackle challenging tasks by making them more accessible and interpretable.\\n\\nIn the context of my architecture, as illustrated in the provided figure, task decomposition is achieved through methods like Chain of Thought or Tree of Thoughts prompting techniques, which help me think step-by-step and decompose tasks into manageable parts.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding chat history"
      ],
      "metadata": {
        "id": "JgFgc6nCi93E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enable conversational context in our system, we need to update our app in two ways:\n",
        "\n",
        "- **Prompt update:** Modify the prompt to support historical messages as input.\n",
        "Contextualizing\n",
        "- **questions:** Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This involves creating a new \"history-aware\" retriever that takes both the query and conversation history as input, and outputs a rephrased query that can be used by the retriever.\n",
        "\n",
        "To implement this, we'll use a prompt with a MessagesPlaceholder variable named \"chat_history\" that allows us to pass in a list of messages to the prompt. We'll also use a helper function called create_history_aware_retriever that manages the case where chat_history is empty and applies a sequence of prompt, LLM, StrOutputParser(), and retriever.\n",
        "\n",
        "The create_history_aware_retriever function constructs a chain that accepts keys input and chat_history as input and has the same output schema as a retriever. This will enable our system to understand user queries that require conversational context to be understood."
      ],
      "metadata": {
        "id": "BWAU0RBVx4fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "contextualize_q_system_prompt = (\"\"\"\n",
        "    Given a chat history and the latest user question\n",
        "    which might reference context in the chat history,\n",
        "    formulate a standalone question which can be understood\n",
        "    without the chat history. Do NOT answer the question,\n",
        "    just reformulate it if needed and otherwise return it as is.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(llm=model, retriever=retriever, prompt=contextualize_q_prompt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKuG7iyTieR6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**create_history_aware_retriever()**\n",
        "- If there is no chat_history, then the input is just passed directly to the retriever.\n",
        "- If there is chat_history, then the prompt and LLM will be used to generate a search query. That search query is then passed to the retriever."
      ],
      "metadata": {
        "id": "A-3QMgMA2ZUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "        ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm=model, prompt=qa_prompt)\n",
        "rag_chain = create_retrieval_chain(retriever=history_aware_retriever, combine_docs_chain=question_answer_chain)\n"
      ],
      "metadata": {
        "id": "H-xD5RXm2kBF"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "chat_history = []\n",
        "question = \"What is Task Decomposition?\"\n",
        "\n",
        "ai_message_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_message_1[\"answer\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "second_question = \"What are common ways of doing it?\"\n",
        "ai_message_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "ai_message_2[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "DuyVD9N-6_u4",
        "outputId": "343d27b0-d30a-4568-f858-227035df5b2a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are three primary methods: \\n\\n- The LLM can be prompted with simple instructions to break a task down into steps, for instance, asking for \"Steps for...\" followed by the task description. \\n\\n- Alternatively, the agent can use more specific instructions designed for the particular task, such as outlining a story. \\n\\n- The final method involves human input, where a human user provides the breakdown steps.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_message_3 = rag_chain.invoke({\"input\": \"any other ways of doing it?\", \"chat_history\": chat_history})\n",
        "ai_message_3[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Lmoen_229Dyo",
        "outputId": "28aa80fc-f609-4905-ac3e-7fae94e67dd2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, another technique is called Chain of Thought (CoT). It's a prompting method that guides the agent to think step-by-step and use test-time computation to solve complex tasks. Essentially, the agent is instructed to verbalize its thought process, which helps it tackle difficult tasks by breaking them down into smaller, more feasible sub-tasks.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stateful management of chat history"
      ],
      "metadata": {
        "id": "5gPAtWQO_PtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To manage chat history in a stateful manner, you can use BaseChatMessageHistory to store the chat history and RunnableWithMessageHistory to handle injecting and updating the chat history."
      ],
      "metadata": {
        "id": "kHA4PMyoBFVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    runnable=rag_chain,\n",
        "    get_session_history=get_session_history,\n",
        "    input_messages_key = \"input\",\n",
        "    history_messages_key = \"chat_history\",\n",
        "    output_messages_key= \"answer\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "tggJrMlO99Pm"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What is Task Decomposition?\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"abc123\"}\n",
        "    },  # constructs a key \"abc123\" in `store`.\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "gnILM8E9DKsn",
        "outputId": "451ee357-34ec-4edf-973b-f8752a97f3be"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.core:Parent run 80320cfd-3051-4c64-ac6b-a197841b1b38 not found for run 2a886c5c-dfa8-4ca1-8b2f-ecb54c2f049f. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is a process that breaks down complex tasks into simpler, more manageable steps. It's a technique used by autonomous agents to plan and execute multi-step tasks more effectively. This approach improves the agent's ability to complete a task by providing a clear structure and allowing for better interpretation of the model's thought process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What are common ways of doing it?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "xAszq6y1DSmv",
        "outputId": "fd006ac2-0aaa-402a-f2cd-dae9705b8283"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.core:Parent run 88eb453f-b5be-40ce-ac88-10ee0d54c7fd not found for run 50f29739-8621-4fed-bfa2-a9ca3feb4759. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are three primary methods: \\n\\n1. The LLM can be prompted with simple instructions to break a task into steps, for instance, asking for \"Steps for XYZ\".\\n\\n2. Using more specific instructions tailored to the task, such as outlining story steps for writing a novel.\\n\\n3. Incorporating human input to guide the task decomposition process. This method often involves human-in-the-loop interactions, where a person provides explicit step-by-step instructions.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## The conversation history can be inspected in the store dict:\n",
        "\n",
        "for message in store[\"abc123\"].messages:\n",
        "    if isinstance(message, AIMessage):\n",
        "        prefix = \"AI\"\n",
        "    else:\n",
        "        prefix = \"User\"\n",
        "\n",
        "    print(f\"{prefix}: {message.content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xHKFKbODWFx",
        "outputId": "63b304e4-0e7f-4adc-a6ed-37126ec5c294"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is Task Decomposition?\n",
            "\n",
            "AI: Task decomposition is a process that breaks down complex tasks into simpler, more manageable steps. It's a technique used by autonomous agents to plan and execute multi-step tasks more effectively. This approach improves the agent's ability to complete a task by providing a clear structure and allowing for better interpretation of the model's thought process.\n",
            "\n",
            "User: What are common ways of doing it?\n",
            "\n",
            "AI: There are three primary methods: \n",
            "\n",
            "1. The LLM can be prompted with simple instructions to break a task into steps, for instance, asking for \"Steps for XYZ\".\n",
            "\n",
            "2. Using more specific instructions tailored to the task, such as outlining story steps for writing a novel.\n",
            "\n",
            "3. Incorporating human input to guide the task decomposition process. This method often involves human-in-the-loop interactions, where a person provides explicit step-by-step instructions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Conversational RAG](https://python.langchain.com/v0.2/assets/images/conversational_retrieval_chain-5c7a96abe29e582bc575a0a0d63f86b0.png)"
      ],
      "metadata": {
        "id": "A9j5E4ihEE8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yoqvnt5WDuuu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDOEnBfhD0WT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}